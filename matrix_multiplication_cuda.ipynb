{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "matrix_multiplication_cuda",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7anasHqf0XmQ",
        "outputId": "831a3c08-5fe3-4fdd-9c64-81351e1b49c4"
      },
      "source": [
        "# importa macro %%cu\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "\n",
        "# carrega plugin\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-qqj_y3j2\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-qqj_y3j2\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=9a9a92c92fce77da33e82dd724e41031b603fd164b9f0e3cc5cab5f22c778d73\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5mznbu7x/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_rrA97nhWqN",
        "outputId": "72ced517-2bf1-4129-b765-ae37d3a2d454"
      },
      "source": [
        "%cd /usr/local\n",
        "%cd cuda-10.0\n",
        "%cd samples\n",
        "%cd 1_Utilities\n",
        "%cd deviceQuery\n",
        "!ls\n",
        "!make\n",
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local\n",
            "/usr/local/cuda-10.0\n",
            "/usr/local/cuda-10.0/samples\n",
            "/usr/local/cuda-10.0/samples/1_Utilities\n",
            "/usr/local/cuda-10.0/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n",
            "/usr/local/cuda-10.0/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda-10.0/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla K80\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.0\n",
            "  CUDA Capability Major/Minor version number:    3.7\n",
            "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
            "  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores\n",
            "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
            "  Memory Clock rate:                             2505 Mhz\n",
            "  Memory Bus Width:                              384-bit\n",
            "  L2 Cache Size:                                 1572864 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Compute Preemption:            No\n",
            "  Supports Cooperative Kernel Launch:            No\n",
            "  Supports MultiDevice Co-op Kernel Launch:      No\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.0, NumDevs = 1\n",
            "Result = PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWAD9sFIwFvq"
      },
      "source": [
        "Universidade Federal de São Carlos\\\n",
        "Departamento de Computação\\\n",
        "Prof. Hélio Crestana Guardia\\\n",
        "Programação Paralela e Distribuída\\\n",
        "\n",
        "**Autores:** Vinicius Henrique dos Santos Carvalho 743602 e João Augusto Leite 743551 \\\n",
        "\n",
        "**Programa :** Multiplicação de matrizes utilizando CUDA\\\n",
        "**Estratégias:** Nesse código, foi abordada a técnica de calcular cada célula\n",
        "em uma thread. Desse modo, a arquitetura tem 1 grid (1,1,1) e bloco contendo\n",
        "uma thread por célula, ou seja (n,n,1).\n",
        "As matrizes foram armazenadas linearmente.\\\n",
        "Para fazer os cálculos no Kernel, primeiro obtém-se a posição da célula\n",
        "que será calculada naquela thread. Para isso, é obtido o número da linha multiplicando o ID do bloco no eixo Y pelo número de elementos de cada linha, possibilitando acessar o primeiro elemento dela e em seguida soma-se o ID da\n",
        "thread no eixo Y. Para achar a coluna, utiliza-se o mesmo cálculo, porém\n",
        "subtituindo os dados de Y por X.\n",
        "Com a posição da célula que será cálculada, é feito um For para percorrer\n",
        "toda a linha da primeira matriz e toda a coluna da segunda, mutiplicando\n",
        "cada um desses elementos e somando, até o final da iteração, que é o limite\n",
        "da matriz.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6hei905VYgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8fdb3e-71bd-46a1-914a-7caff394a19c"
      },
      "source": [
        "# %%gpu\n",
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define DIMENSAO 32\n",
        "#define MAX_VALUE 9\n",
        "\n",
        "//Função para iniciar a matriz com valores aleatórios até o MAX_VALUE definido\n",
        "void init_matrix(int width, int height,int *m)\n",
        "{\n",
        "    int i, j;\n",
        "    for(i=0; i<width; i++){\n",
        "        for(j=0; j<height; j++){\n",
        "            m[i*width+j] = rand() % MAX_VALUE;\n",
        "            }\n",
        "    }\n",
        "}\n",
        "\n",
        "void print_matrix(int width, int height, int *m)\n",
        "{\n",
        "    int i, j;\n",
        "\n",
        "    for(i=0; i<width; i++)\n",
        "    {\n",
        "        printf(\"\\n%d: \", i);\n",
        "        for(j=0; j<height; j++)\n",
        "            printf(\"%d \", m[i*width+j]);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "//Kernel CUDA que faz a mutiplicação\n",
        "__global__ void MultiplicaMatriz(int *a, int *b, int *c, int n)\n",
        "{\n",
        "  //Cálculo para obter a posição do elemento da matriz no vetor\n",
        "  //(blockIdx.x,blockIdx.y) são as coordenadas da thread no bloco\n",
        "  //que também representa a célula que será calculada naquela thread\n",
        "\tint lin = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        " \n",
        "\tint soma = 0;\n",
        "\tfor (int k = 0; k < n; k++) {\n",
        "\t\tsoma += a[lin*n + k] * b[k * n + col];\n",
        "\t}\n",
        "\tc[lin*n + col] = soma;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    \n",
        "    int n = DIMENSAO;\n",
        "    int tamanho = n*n;\n",
        "    int *g_m1, *g_m2, *g_r;\n",
        "    int m1[tamanho], m2[tamanho], resultado[tamanho];\n",
        "\n",
        "    //Aloca memória das matrizes na GPU\n",
        "\t  cudaMalloc(&g_m1, sizeof(int)*tamanho);\n",
        "\t  cudaMalloc(&g_m2, sizeof(int)*tamanho);\n",
        "\t  cudaMalloc(&g_r, sizeof(int)*tamanho);\n",
        "\n",
        "    printf(\"\\nMatriz 1:\\n\");\n",
        "    init_matrix(n, n, m1);\n",
        "    print_matrix(n, n, m1);\n",
        "    \n",
        "    printf(\"\\nMatriz 2:\\n\");\n",
        "    init_matrix(n, n, m2);\n",
        "    print_matrix(n, n, m2);\n",
        "    \n",
        "    //Copia as matrizes do host para a GPU\n",
        "    cudaMemcpy(g_m1, m1, sizeof(int)*tamanho, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(g_m2, m2, sizeof(int)*tamanho, cudaMemcpyHostToDevice);\n",
        "\n",
        "    //Define a dimensão do bloco, nesse caso NxN threads\n",
        "    dim3 threadsPerBlock(n, n, 1);\n",
        "    //Define a dimensão da grade, nesse caso, somente uma será usada\n",
        "\t  dim3 blocksPerGrid(1, 1, 1);\n",
        "\n",
        "    //Inicia o Kernel passando as dimensões dos blocos e grid, as duas matrizes \n",
        "    //que serão mutiplicadas, a matriz que armazenará o resultado e o \n",
        "    //número elementos por linha/coluna\n",
        "    MultiplicaMatriz<<<dimGrid, threadsPerBlock>>>(g_m1, g_m2, g_r, n);\n",
        "\n",
        "    //Verifica se houve algum erro ao ativar o Kernel\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "      printf(\"CUDA Error: %s\\n\",cudaGetErrorString(err));\n",
        "    }\n",
        "\n",
        "    //Função para o Host aguardar a execução de todas as threads\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    //Copia a matriz resultante da GPU para o HOST\n",
        "    cudaMemcpy(resultado, g_r, sizeof(int)*tamanho, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nMatriz resultado:\\n\");\n",
        "    print_matrix(n, n, resultado);\n",
        "\n",
        "    //DLibera a memória alocada na GPU e a reseta\n",
        "    cudaFree(g_m1);\n",
        "    cudaFree(g_m2);\n",
        "    cudaFree(g_r);\n",
        "\n",
        "    cudaDeviceReset();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Matriz 1:\n",
            "\n",
            "0: 1 7 0 7 5 7 1 3 6 1 5 4 5 7 5 4 6 0 7 1 8 8 6 6 8 8 8 4 1 1 5 0 \n",
            "1: 0 3 5 3 1 7 4 7 6 0 0 2 5 4 5 2 2 3 2 1 1 8 8 0 5 5 4 4 6 0 5 6 \n",
            "2: 2 8 7 3 4 2 0 0 0 0 2 6 2 5 6 5 7 6 6 8 5 3 6 2 8 1 6 6 8 0 1 1 \n",
            "3: 7 0 3 2 0 1 2 1 8 3 5 2 6 0 7 2 7 2 8 1 6 5 1 5 4 6 0 4 6 2 3 2 \n",
            "4: 0 4 3 7 5 3 6 5 4 2 5 2 1 3 2 8 3 2 0 0 7 2 4 3 6 2 5 1 2 6 4 2 \n",
            "5: 2 7 8 5 1 5 1 4 8 4 6 7 5 8 6 0 8 4 0 7 4 2 8 1 5 2 3 7 8 7 8 1 \n",
            "6: 3 7 5 2 3 6 6 0 2 1 7 7 8 2 5 7 7 4 3 0 6 0 2 0 2 3 6 2 8 5 1 2 \n",
            "7: 1 6 2 2 2 7 3 2 6 8 7 3 0 4 2 5 8 3 5 3 4 5 4 4 8 1 6 7 4 6 7 5 \n",
            "8: 3 8 7 3 6 8 5 1 6 2 5 6 6 5 0 3 8 5 6 1 2 8 6 8 0 1 7 2 7 3 7 2 \n",
            "9: 2 4 3 6 1 0 8 7 2 2 2 6 7 2 0 6 6 4 6 6 4 3 5 2 4 1 3 3 5 8 3 5 \n",
            "10: 3 6 1 3 4 7 8 4 0 2 1 7 2 8 2 6 2 8 3 4 0 7 4 5 8 7 6 2 7 0 8 8 \n",
            "11: 4 0 2 7 7 2 2 5 2 2 3 2 8 3 0 1 3 1 5 1 8 8 6 8 6 1 1 4 8 7 4 4 \n",
            "12: 5 4 2 1 4 2 6 6 4 7 7 2 2 7 3 3 6 7 4 6 6 0 3 3 1 2 6 1 1 8 3 6 \n",
            "13: 1 3 8 6 5 3 3 8 2 1 1 4 6 2 5 4 0 0 8 4 7 2 6 0 4 3 8 3 0 2 8 1 \n",
            "14: 5 5 7 8 8 0 7 1 8 8 3 6 2 8 8 0 7 5 5 5 7 2 3 0 3 2 4 3 2 1 2 7 \n",
            "15: 6 1 7 5 8 3 5 7 3 8 2 3 6 1 3 2 6 6 5 2 8 0 3 0 0 5 3 3 6 6 8 3 \n",
            "16: 5 6 6 4 1 2 2 2 0 5 5 4 4 6 6 0 4 2 0 1 0 1 2 1 6 3 2 1 7 1 2 1 \n",
            "17: 6 7 5 5 0 6 5 7 0 8 2 2 5 6 0 7 7 1 7 7 2 0 6 7 1 8 6 7 8 0 8 3 \n",
            "18: 7 5 8 5 0 4 4 0 3 4 0 6 0 1 5 7 2 3 3 2 1 1 0 0 7 7 7 4 5 7 7 1 \n",
            "19: 1 4 6 1 6 8 8 7 2 6 5 2 7 8 7 7 0 1 8 8 0 8 8 8 4 7 1 7 3 0 8 4 \n",
            "20: 2 4 3 0 3 0 7 3 6 1 5 3 0 3 8 7 3 7 6 3 5 4 2 7 0 2 6 3 2 3 5 2 \n",
            "21: 7 6 2 0 4 1 3 8 0 7 2 1 8 2 6 2 7 4 6 3 8 6 2 6 8 8 0 8 2 3 2 8 \n",
            "22: 0 2 8 2 1 0 1 2 7 2 3 7 2 0 7 0 2 2 4 8 0 6 5 6 3 3 4 3 6 4 2 4 \n",
            "23: 6 8 6 8 0 8 1 5 1 2 1 3 0 0 1 3 2 5 0 0 0 6 5 1 7 0 5 5 4 5 0 1 \n",
            "24: 3 5 7 1 2 8 6 3 1 8 4 0 6 5 3 6 0 1 7 0 5 3 2 4 1 5 0 3 8 7 4 2 \n",
            "25: 1 3 3 3 0 8 6 0 7 8 0 4 5 1 1 5 2 6 3 8 7 3 3 6 6 1 0 6 6 5 6 8 \n",
            "26: 6 8 2 6 7 7 4 3 6 4 5 0 5 4 3 8 0 7 5 7 1 8 5 8 7 3 3 4 8 7 3 3 \n",
            "27: 6 6 8 2 4 3 5 8 8 8 0 2 4 3 1 4 1 6 0 1 3 3 7 1 7 8 6 4 6 7 6 4 \n",
            "28: 4 5 4 6 6 8 6 3 7 4 6 0 7 7 2 7 3 3 6 4 4 4 6 0 3 1 5 0 8 0 2 2 \n",
            "29: 5 7 6 0 6 1 4 2 5 1 3 2 6 5 0 7 6 6 3 0 8 7 0 0 8 3 0 5 3 1 5 6 \n",
            "30: 6 3 7 1 4 2 3 8 3 4 1 7 8 1 6 5 5 0 3 2 7 4 2 4 5 2 7 0 1 4 6 7 \n",
            "31: 5 2 8 7 4 1 6 5 5 7 4 4 6 8 8 0 8 2 2 4 4 4 6 1 5 2 8 6 6 5 3 0 \n",
            "Matriz 2:\n",
            "\n",
            "0: 6 0 8 1 1 5 5 5 2 7 0 8 6 8 7 5 0 0 7 4 5 4 5 8 4 2 5 2 6 6 0 3 \n",
            "1: 7 8 2 6 3 7 2 5 5 1 2 2 7 0 5 7 8 3 3 2 5 6 1 1 0 4 1 6 2 8 7 7 \n",
            "2: 8 0 4 0 8 5 5 2 6 5 5 4 4 8 1 3 1 4 3 4 8 4 3 6 8 2 1 1 2 8 6 1 \n",
            "3: 7 2 1 6 5 4 6 2 7 0 4 2 0 5 3 1 7 6 3 5 8 7 2 8 0 2 7 2 8 5 1 6 \n",
            "4: 5 2 1 1 4 8 1 3 8 3 3 6 0 5 5 5 0 0 1 0 7 2 6 5 4 4 8 1 7 0 8 3 \n",
            "5: 1 7 2 5 4 1 8 4 5 1 8 3 6 5 8 6 5 1 6 1 3 3 6 5 6 3 6 4 4 3 6 3 \n",
            "6: 2 6 8 4 8 6 8 2 7 6 5 2 2 2 8 5 3 4 4 4 5 1 0 0 3 5 3 5 8 0 8 8 \n",
            "7: 6 7 4 3 2 1 3 7 7 8 0 7 2 7 3 3 2 7 8 5 7 6 4 8 2 7 4 0 7 3 8 2 \n",
            "8: 8 1 4 2 1 7 0 8 5 1 5 5 6 6 8 6 3 5 2 1 3 6 7 5 4 2 5 0 5 3 1 2 \n",
            "9: 4 5 4 5 3 3 3 6 2 8 2 8 3 2 5 6 5 5 5 8 1 3 5 5 5 8 4 8 2 5 2 5 \n",
            "10: 1 4 8 2 7 0 0 7 6 0 6 1 2 0 7 6 6 4 5 7 7 8 1 2 8 3 1 8 8 1 2 7 \n",
            "11: 6 2 1 2 2 8 1 0 8 5 1 0 4 8 6 1 1 2 6 0 0 5 2 8 0 1 5 8 1 8 7 7 \n",
            "12: 1 6 7 1 5 6 1 2 1 0 2 5 7 8 4 8 0 1 8 0 6 8 6 6 8 2 6 0 8 2 5 7 \n",
            "13: 8 3 0 2 8 8 4 0 7 5 3 5 4 7 4 2 8 2 0 3 8 4 8 7 5 3 7 4 5 3 3 2 \n",
            "14: 5 1 4 2 0 6 0 7 2 3 1 5 1 6 5 7 6 6 8 5 1 7 4 6 1 0 0 4 4 3 6 7 \n",
            "15: 4 8 0 2 6 0 8 8 3 0 2 2 4 8 7 1 5 4 5 4 3 0 0 2 0 0 7 2 1 4 0 3 \n",
            "16: 2 0 5 8 7 2 5 8 1 6 1 5 5 6 5 8 2 1 3 3 1 1 3 8 1 1 0 0 4 0 3 6 \n",
            "17: 8 7 3 4 0 6 4 1 3 5 5 8 1 1 5 1 0 7 2 8 8 5 7 8 7 7 8 0 6 3 4 3 \n",
            "18: 1 7 7 8 2 2 1 6 6 4 3 5 3 0 6 3 7 8 2 4 2 7 3 7 6 3 7 1 6 2 2 7 \n",
            "19: 7 0 6 8 1 5 3 5 7 6 1 1 4 7 4 0 4 6 5 4 5 8 3 0 2 1 8 6 2 1 2 7 \n",
            "20: 8 7 4 0 3 7 5 0 3 4 1 7 0 4 6 4 8 2 0 2 8 1 2 0 0 8 6 2 7 0 8 7 \n",
            "21: 7 3 5 8 0 0 6 3 4 8 8 5 1 5 7 0 5 5 3 5 6 3 3 7 1 7 7 8 7 4 6 3 \n",
            "22: 8 3 1 8 1 5 0 5 2 8 8 3 5 7 4 1 1 5 4 8 8 7 4 0 6 2 7 2 7 4 4 4 \n",
            "23: 5 3 1 6 8 1 1 2 0 0 3 3 5 7 3 7 1 7 4 8 4 8 6 8 8 4 8 4 0 3 6 3 \n",
            "24: 4 7 8 2 6 7 2 5 5 5 8 2 2 2 7 3 8 2 2 3 8 0 2 5 4 1 1 2 3 7 4 5 \n",
            "25: 4 3 7 1 8 0 4 4 6 4 4 6 4 2 0 3 2 3 4 8 1 6 5 5 6 6 6 7 2 1 3 6 \n",
            "26: 2 0 6 1 7 1 5 4 3 1 8 6 1 0 0 4 1 5 3 2 0 6 5 4 3 2 2 4 1 4 1 3 \n",
            "27: 4 5 2 2 5 8 5 6 7 4 3 6 2 4 8 3 7 1 5 7 7 2 1 0 2 3 4 4 5 3 7 7 \n",
            "28: 0 1 1 3 7 4 0 3 6 2 0 0 4 7 3 2 8 7 7 6 0 6 6 2 1 1 6 4 5 3 3 3 \n",
            "29: 2 2 4 0 6 2 1 1 4 8 1 8 6 3 8 5 1 5 3 1 2 7 1 1 0 6 6 3 0 7 4 2 \n",
            "30: 0 8 0 6 1 8 7 4 7 7 1 5 1 8 1 2 2 2 1 2 1 2 4 8 6 8 2 6 4 6 6 4 \n",
            "31: 3 4 8 4 3 4 6 2 2 8 5 1 7 6 3 0 0 2 2 8 5 4 7 0 1 0 7 5 4 2 0 7 \n",
            "Matriz resultado:\n",
            "\n",
            "0: 661 591 545 543 617 596 489 599 667 505 572 613 456 644 700 569 628 525 506 541 651 659 545 705 509 496 685 518 658 486 627 699 \n",
            "1: 521 457 437 431 434 491 419 463 540 502 477 455 397 594 521 367 421 444 460 483 530 517 479 519 406 369 542 385 552 406 506 504 \n",
            "2: 615 446 452 477 521 606 394 505 592 485 430 468 408 599 595 420 552 489 472 506 585 566 447 541 394 325 574 417 518 471 532 615 \n",
            "3: 460 389 518 384 441 450 328 499 451 435 336 502 391 544 574 474 434 435 460 480 439 526 431 557 416 358 507 359 525 345 425 553 \n",
            "4: 498 464 389 341 509 452 411 427 517 411 401 448 324 498 547 404 430 415 375 411 528 446 353 449 329 390 504 356 486 389 472 489 \n",
            "5: 714 518 523 538 619 748 458 609 737 646 488 619 549 788 730 573 600 565 595 583 679 735 578 675 523 484 654 538 653 597 656 697 \n",
            "6: 469 472 489 354 580 520 411 470 542 403 392 471 450 558 622 522 462 423 514 424 484 550 419 500 395 350 523 415 533 423 518 610 \n",
            "7: 568 580 541 545 592 575 500 617 631 593 526 583 481 602 735 538 585 521 493 578 580 580 501 590 472 485 614 538 568 512 557 657 \n",
            "8: 627 545 510 587 635 613 507 553 665 550 549 591 539 719 703 583 496 554 538 547 620 688 592 723 575 499 711 516 651 531 645 642 \n",
            "9: 542 510 520 476 533 534 441 475 589 568 383 500 450 619 629 424 439 516 501 495 572 588 412 525 370 414 632 402 571 446 521 624 \n",
            "10: 616 616 533 556 602 620 573 508 696 620 548 533 479 683 653 431 521 535 530 616 618 601 584 674 500 484 729 577 605 518 604 663 \n",
            "11: 541 481 474 448 531 531 385 426 554 532 417 542 406 651 619 450 463 478 466 506 629 583 488 579 436 459 689 390 629 418 559 563 \n",
            "12: 578 485 551 435 530 535 435 505 563 590 398 604 444 558 635 503 426 502 462 534 573 601 501 541 449 478 615 444 558 404 496 598 \n",
            "13: 540 492 452 395 455 537 423 467 597 485 408 521 336 584 505 403 430 461 444 410 555 549 411 551 422 412 541 362 547 438 547 558 \n",
            "14: 729 461 580 490 553 732 477 541 675 621 462 618 446 667 691 535 538 519 484 564 665 627 565 656 456 451 653 483 671 473 573 711 \n",
            "15: 568 515 525 424 537 605 478 510 629 616 365 666 420 663 612 519 420 494 519 527 577 587 528 640 485 535 644 407 639 444 588 615 \n",
            "16: 410 310 367 295 427 430 275 361 428 368 292 357 328 448 416 376 389 333 401 388 408 439 359 461 338 267 354 354 400 384 377 436 \n",
            "17: 607 602 548 604 685 582 576 639 687 610 447 613 553 747 636 547 571 579 623 677 602 691 570 675 560 504 718 550 614 517 583 709 \n",
            "18: 488 422 454 340 515 504 458 466 564 479 381 496 396 540 545 408 435 430 459 441 416 487 386 542 363 366 482 436 398 531 441 520 \n",
            "19: 694 686 605 640 659 661 567 672 775 676 583 640 544 814 762 568 612 629 657 713 741 743 620 715 664 555 808 627 723 520 728 759 \n",
            "20: 530 453 441 407 453 482 392 482 510 444 379 504 347 517 574 438 414 505 420 490 476 533 416 516 399 407 538 404 489 367 496 539 \n",
            "21: 624 602 654 491 554 594 472 581 570 659 419 671 490 647 691 564 519 492 567 641 662 604 533 635 483 523 661 480 637 446 628 721 \n",
            "22: 513 288 426 404 402 451 269 427 492 453 376 390 378 550 480 356 359 488 439 465 423 582 410 476 370 297 502 411 404 410 436 492 \n",
            "23: 490 386 356 364 396 394 386 395 458 410 411 411 348 449 512 339 395 384 415 409 496 440 333 463 295 319 444 329 420 475 418 408 \n",
            "24: 461 535 457 391 564 463 429 456 549 480 377 527 449 562 603 490 501 470 493 501 503 543 448 502 470 461 591 446 519 430 526 542 \n",
            "25: 570 570 502 505 507 606 494 486 575 562 449 516 490 653 691 456 504 505 491 552 570 557 506 507 447 457 692 482 536 462 540 639 \n",
            "26: 710 629 588 592 634 637 515 635 715 591 571 641 558 733 816 569 613 644 616 662 739 753 608 682 562 525 841 549 690 569 621 688 \n",
            "27: 671 558 560 429 588 632 495 566 657 662 504 673 509 677 653 512 458 538 546 595 639 622 570 623 519 543 663 470 595 573 592 585 \n",
            "28: 584 526 512 486 565 561 466 554 628 466 500 523 463 639 670 502 551 512 516 503 630 610 536 577 496 407 671 421 674 422 505 600 \n",
            "29: 571 545 504 379 502 593 472 468 555 514 414 531 413 583 634 439 470 367 394 451 626 425 446 539 408 427 562 377 578 421 532 568 \n",
            "30: 563 486 551 392 505 550 454 500 536 571 391 571 449 687 583 502 380 449 522 452 535 560 486 627 420 424 564 420 532 484 562 595 \n",
            "31: 679 448 582 480 641 677 470 584 678 635 486 659 459 721 689 576 554 556 585 578 662 678 554 689 499 469 622 485 677 524 614 673 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}